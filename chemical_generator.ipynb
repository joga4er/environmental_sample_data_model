{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import types\n",
    "from linkml_runtime.dumpers import yaml_dumper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data upload file path\n",
    "raw_filepaths = [\n",
    "    r'C:/Users/johanna.ganglbauer/archive/code/PFAS-SCIEX-Data-Processing-Analysis/example_data_raw/test_data_core.txt',\n",
    "    r'C:/Users/johanna.ganglbauer/archive/code/PFAS-SCIEX-Data-Processing-Analysis/example_data_raw/test_data_extended.txt',\n",
    "]\n",
    "\n",
    "# file paths for IDL and IQL data - not meant to be adopted\n",
    "idl_2024_filepath = r'C:/Users/johanna.ganglbauer/archive/code/PFAS-SCIEX-Data-Processing-Analysis/example_data_raw/IDL_2024.csv'\n",
    "idl_iql_filepath = r'C:/Users/johanna.ganglbauer/archive/code/PFAS-SCIEX-Data-Processing-Analysis/example_data_raw/IDL_2024.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define output file locations\n",
    "output_schema_xlsx = r'outputs/schema.xlsx'\n",
    "output_data_yaml = r'outputs/data.yaml'\n",
    "output_data_csv = r'outputs/data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load linkML data model, get illstrations and python data classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_python_schema(yamlschema: str, pythonschema: str) -> types.ModuleType:\n",
    "    \"\"\"Creates python schema from yaml schema and imports python script.\n",
    "\n",
    "    :param yamlschema: Name of .yaml schema.\n",
    "    :type yamlschema: str\n",
    "    :param data: Name of .py schema to be created.\n",
    "    :type data: str\n",
    "    \"\"\"  \n",
    "    my_env = os.environ.copy()\n",
    "\n",
    "    p = subprocess.Popen(\n",
    "        ['gen-python', yamlschema, '>', pythonschema], shell=True, stdout=subprocess.PIPE, env=my_env,\n",
    "        cwd=os.getcwd())\n",
    "    _, error = p.communicate()\n",
    "    if error is not None:\n",
    "        print(error)\n",
    "        return None\n",
    "    else:\n",
    "        print(f'Suceeded in converting yaml schema to python.')\n",
    "    \n",
    "def generate_schema_png(yamlschema: str) -> None:\n",
    "    \"\"\"Generates sketch of data schema.\n",
    "\n",
    "    :param yamlschema: Name of .yaml schema.\n",
    "    :type yamlschema: str\n",
    "    \"\"\"\n",
    "    my_env = os.environ.copy()\n",
    "\n",
    "    p = subprocess.Popen(\n",
    "        ['gen-yuml', '-f', 'png', '-d', 'outputs', '--diagram-name', 'schema', yamlschema], shell=True, stdout=subprocess.PIPE, env=my_env,\n",
    "        cwd=os.getcwd())\n",
    "    _, error = p.communicate()\n",
    "    if error is not None:\n",
    "        print(error)\n",
    "    else:\n",
    "        print(f'Suceeded in creating image for schema.')\n",
    "\n",
    "def generate_schema_excel(yamlschema: str) -> None:\n",
    "    \"\"\"Generates excel file representing data schema.\n",
    "\n",
    "    :param yamlschema: Name of .yaml schema.\n",
    "    :type yamlschema: str\n",
    "    \"\"\"\n",
    "    my_env = os.environ.copy()\n",
    "\n",
    "    p = subprocess.Popen(\n",
    "        ['gen-excel', yamlschema, '--output', 'outputs/schema.xlsx'], shell=True, stdout=subprocess.PIPE, env=my_env,\n",
    "        cwd=os.getcwd())\n",
    "    _, error = p.communicate()\n",
    "    if error is not None:\n",
    "        print(error)\n",
    "    else:\n",
    "        print(f'Suceeded in creating spreadsheet for schema.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suceeded in converting yaml schema to python.\n",
      "Suceeded in creating image for schema.\n"
     ]
    }
   ],
   "source": [
    "create_python_schema(yamlschema='chemicals.yaml', pythonschema='chemicals.py')\n",
    "import chemicals as pyda\n",
    "generate_schema_png(yamlschema='chemicals.yaml')\n",
    "generate_schema_excel(yamlschema='chemicals.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in and data clean up\n",
    "- reads in mass spec data results and uses the calibration data to extract relevant information\n",
    "- reads in excel with associated names and assigns them to PFAS components\n",
    "- read in IDLs and assigns them to PFAS components\n",
    "\n",
    "-> combines the data and saves it to the python data classes deduced from linkml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johanna.ganglbauer\\AppData\\Local\\Temp\\ipykernel_23508\\2505820438.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  this_data['Sample Name'][~mask_names] = [compound + ' Core' for compound in this_data['Sample Name'][~mask_names].to_list()]\n",
      "C:\\Users\\johanna.ganglbauer\\AppData\\Local\\Temp\\ipykernel_23508\\2505820438.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  this_data['Sample Name'][~mask_names] = [compound + ' Ext' for compound in this_data['Sample Name'][~mask_names].to_list()]\n",
      "C:\\Users\\johanna.ganglbauer\\AppData\\Local\\Temp\\ipykernel_23508\\2505820438.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Component Name'][mask_names] = [compound + ' MS' for compound in data['Component Name'][mask_names].to_list()]\n",
      "C:\\Users\\johanna.ganglbauer\\AppData\\Local\\Temp\\ipykernel_23508\\2505820438.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Component Name'][mask_names] = [compound[:-3] + ' MS' for compound in data['Component Name'][mask_names].to_list()]\n"
     ]
    }
   ],
   "source": [
    "# Define columns of input which are needed for further processes:\n",
    "columns_considered = [\n",
    "    'Sample Name', 'Sample Index', 'Sample Comment', 'Sample Type',\n",
    "    'Component Name',  'Component Group Name', 'IS Name', \n",
    "    'Expected RT',\n",
    "    'Precursor Mass', 'Fragment Mass',\n",
    "]\n",
    "\n",
    "# Load input data files and put them all in one dataframe\n",
    "data = pd.DataFrame()  # initialize empty data frames\n",
    "sample_index = 0  # initialize Component Index\n",
    "for file in raw_filepaths:\n",
    "    # read in file\n",
    "    if file[-4:] == '.csv':\n",
    "        this_data = pd.read_csv(file, delimiter=',', encoding='utf-8', low_memory=False, header=0,)\n",
    "    elif file[-4:] == '.txt':\n",
    "        this_data = pd.read_csv(file, delimiter='\\t', encoding='utf-8', low_memory=False, header=0,)\n",
    "    else:\n",
    "        print('Raw input file paths must either be .csv or .txt files.')\n",
    "\n",
    "    # increase component index to remain unique for multiple data frames\n",
    "    this_data['Sample Index'] = this_data['Sample Index'] + sample_index\n",
    "    sample_index += max(this_data['Sample Index'])\n",
    "    check = this_data['Sample Index'].value_counts()\n",
    "\n",
    "    # make sure each sample name ends with Ext for extended method and with Core for core method\n",
    "    if file[-12:-4] == 'extended':\n",
    "        mask_names = this_data['Sample Name'].str.endswith('Ext')\n",
    "        this_data['Sample Name'][~mask_names] = [compound + ' Ext' for compound in this_data['Sample Name'][~mask_names].to_list()]\n",
    "\n",
    "        # checks if extended data has 107 channels\n",
    "        for index, elem in check[(check != 107)].items():\n",
    "            print(f'The sample with Index {index} has {elem} channels.')\n",
    "\n",
    "    elif file[-8:-4] == 'core':\n",
    "        mask_names = this_data['Sample Name'].str.endswith('Core')\n",
    "        this_data['Sample Name'][~mask_names] = [compound + ' Core' for compound in this_data['Sample Name'][~mask_names].to_list()]\n",
    "\n",
    "        # checks if extended data has 107 channels\n",
    "        for index, elem in check[(check != 141)].items():\n",
    "            print(f'The sample with Index {index} has {elem} channels.')\n",
    "    else:\n",
    "        print('If you combine core method and extended method make sure your input file names end with _core and _extended respectively.')\n",
    "\n",
    "    # append actual dataframe in list (this_data) to huge dataframe (data)\n",
    "    if data.empty:\n",
    "        data = this_data[columns_considered]  # initialize data in first step (when data is empty)\n",
    "    else:\n",
    "        data = pd.concat([data, this_data[columns_considered]], ignore_index=True)  # append to data\n",
    "\n",
    "# Correct channel names in original data (all of the TOF channels are labelled by _TOF MS, only 2 of them are labeled by only _TOF)\n",
    "mask_names = data['Component Name'].str.endswith('_TOF')\n",
    "data['Component Name'][mask_names] = [compound + ' MS' for compound in data['Component Name'][mask_names].to_list()]\n",
    "\n",
    "# some have an underscore between TOF and MS, this is removed\n",
    "mask_names = data['Component Name'].str.endswith('_TOF_MS')\n",
    "data['Component Name'][mask_names] = [compound[:-3] + ' MS' for compound in data['Component Name'][mask_names].to_list()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First make sure that each sample of the core method is assigned correctly to a corresponding sample of the extended method.\\\n",
    "If duplicates of samples with the exact same name are available the assignment works just by the order or the sample index.\n",
    "\n",
    "Then assign each PFAS component detection channels of precursor masses (_TOF MS) to the channel detecting its fragements (default).\n",
    "Currently, some PFAS components are only detected in the _TOF MS channel, then the corresponding fragments are set to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one of the sample PFAS CS0 0ng/ml Ext has no related Core Method. Find out if this is a problem.\n",
      "The sample PFAS QCS0 0ng/ml Ext has no related Core Method. Find out if this is a problem.\n"
     ]
    }
   ],
   "source": [
    "# make sample index equal if one sample exist as both \"Core Method\" and \"Extended Method\"\n",
    "sample_names = data['Sample Name'].unique()\n",
    "\n",
    "# combine indices of Core Method and Extended Method if both are available\n",
    "detect = 0\n",
    "index_mapper = {}\n",
    "for sample_name in sample_names:\n",
    "    if sample_name[-3:] == 'Ext':\n",
    "        if sample_name[:-3] + 'Core' in sample_names:\n",
    "            sample_indices_core = data.loc[data['Sample Name'].isin([sample_name[:-3] + 'Core']), 'Sample Index'].value_counts().index\n",
    "            sample_indices_extended = data.loc[data['Sample Name'].isin([sample_name]), 'Sample Index'].value_counts().index\n",
    "            if len(sample_indices_core) != len(sample_indices_extended):\n",
    "                print(f'At least one of the sample {sample_name} has no related Core Method. Find out if this is a problem.')\n",
    "            for index_core, index_extended in zip(sample_indices_core, sample_indices_extended):\n",
    "                data.loc[data['Sample Index'].isin([index_core, index_extended]), 'Sample Index'] = index_core\n",
    "                index_mapper[index_core] = sample_name[:-3]\n",
    "                detect+=1\n",
    "        else:\n",
    "            print(f'The sample {sample_name} has no related Core Method. Find out if this is a problem.')\n",
    "    elif sample_name[-4:] == 'Core':\n",
    "        continue\n",
    "    else:\n",
    "        print(f'Make sure your input data name ends either with _core or with _extended.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PFPrA', 'PFBA', 'PFPeA', 'PFHxA', 'PFHpA', 'PFOA', 'PFNA', 'PFDA', 'PFUdA', 'PFDoA', 'PFTrDA', 'PFTeDA', '3:3 FTCA', '5:3 FTCA', '7:3 FTCA', 'FOEA', 'PFPrS', 'PFBS', 'PFPeS', 'L-PFHxS', 'Br-PFHxS', 'PFHpS', 'L-PFOS', 'Br-PFOS', 'PFNS', 'PFDS', 'MeFBSA', 'N-EtFBSA-M', 'FBSA', 'FPeSA', 'FHxSA', 'FHpSA', 'FOSA', 'L-N-MeFOSAA', 'Br-N-MeFOSAA', 'L-N-EtFOSAA', 'Br-N-EtFOSAA', 'MeFOSA', 'EtFOSA', 'HFPO-DA', 'DONA', '9ClPF3ONS', '11ClPF3OUdS', '3,6-OPFHpA', 'PF4OPeA', 'PF5OHxA', 'PFECHS', 'PFEESA', 'N-AP-FHxSA', 'N-TAmP-FHxSA', '4:2 FTS', '6:2 FTS', '8:2 FTS', '10:2 FTS'] 54\n",
      "['PFPrA_TOF MS', 'PFBA_TOF MS', 'PFPeA_TOF MS', 'PFHxA_TOF MS', 'PFHpA_TOF MS', 'PFOA_TOF MS', 'PFNA_TOF MS', 'PFDA_TOF MS', 'PFUdA_TOF MS', 'PFDoA_TOF MS', 'PFTrDA_TOF MS', 'PFTeDA_TOF MS', '3:3 FTCA_TOF MS', '5:3 FTCA_TOF MS', '7:3 FTCA_TOF MS', 'FOEA_TOF MS', 'PFPrS_TOF MS', 'PFBS_TOF MS', 'PFPeS_TOF MS', 'L-PFHxS_TOF MS', 'Br-PFHxS_TOF MS', 'PFHpS_TOF MS', 'L-PFOS_TOF MS', 'Br-PFOS_TOF MS', 'PFNS_TOF MS', 'PFDS_TOF MS', 'MeFBSA_TOF MS', 'N-EtFBSA-M_TOF MS', 'FBSA_TOF MS', 'FPeSA_TOF MS', 'FHxSA_TOF MS', 'FHpSA_TOF MS', 'FOSA_TOF MS', 'L-N-MeFOSAA_TOF MS', 'Br-N-MeFOSAA_TOF MS', 'L-N-EtFOSAA_TOF MS', 'Br-N-EtFOSAA_TOF MS', 'MeFOSA_TOF MS', 'EtFOSA_TOF MS', 'HFPO-DA_TOF MS', 'DONA_TOF MS', '9ClPF3ONS_TOF MS', '11ClPF3OUdS_TOF MS', '3,6-OPFHpA_TOF MS', 'PF4OPeA_TOF MS', 'PF5OHxA_TOF MS', 'PFECHS_TOF MS', 'PFEESA_TOF MS', 'N-AP-FHxSA_TOF MS', 'N-TAmP-FHxSA_TOF MS', '4:2 FTS_TOF MS', '6:2 FTS_TOF MS', '8:2 FTS_TOF MS', '10:2 FTS_TOF MS'] 54\n",
      "The standard: 13C2_PFOA_TOF MS has no corresponding IDA or IPS in the default MS channel. It is ignored in the following calculations.\n"
     ]
    }
   ],
   "source": [
    "# Get the order of components and split it to default channel (MS/MS) and TOF channel.\n",
    "# If either channel is not available a copy of the other one is used respectively.\n",
    "first_sample_id = data['Sample Index'].value_counts().index[0]  # index of first sample\n",
    "components_filtered = data.loc[data['Sample Index'] == first_sample_id, 'Component Name']  # channel names of first sample\n",
    "components_sorted = components_filtered[~components_filtered.str.contains('IDA|IPS|13C|d-|d3-|d5-')].to_list()  # channel names excluding IPS and IDA\n",
    "ida_ips_sorted = components_filtered[components_filtered.str.contains('IDA|IPS|13C|d-|d3-|d5-')].to_list()  # channel names excluding IPS and IDA\n",
    "\n",
    "# initialize and fill lists of sorted components\n",
    "components_fragmented = []\n",
    "components_precursor = []\n",
    "skip_components = []\n",
    "for component in components_sorted:\n",
    "    if component in skip_components:\n",
    "        continue\n",
    "    if '_TOF MS' in component:\n",
    "        components_precursor.append(component)\n",
    "        skip_components.append(component)\n",
    "        if component[:-7] in components_sorted:\n",
    "            components_fragmented.append(component[:-7])\n",
    "        else:\n",
    "            components_fragmented.append(np.nan)\n",
    "        skip_components.append(component[:-7])\n",
    "    else:\n",
    "        components_fragmented.append(component)\n",
    "        skip_components.append(component)\n",
    "        if component + '_TOF MS' in components_sorted:\n",
    "            components_precursor.append(component + '_TOF MS')\n",
    "        else:\n",
    "            components_precursor.append(np.nan)\n",
    "        skip_components.append(component + '_TOF MS')\n",
    "\n",
    "print(components_fragmented, len(components_fragmented))\n",
    "print(components_precursor, len(components_precursor))\n",
    "\n",
    "# initialize and fill lists of sorted internal standards\n",
    "ida_ips_fragmented = []\n",
    "ida_ips_precursor = []\n",
    "skip_standards = []\n",
    "for standard in ida_ips_sorted:\n",
    "    if standard in skip_standards:\n",
    "        continue\n",
    "    if '_TOF MS' not in standard:\n",
    "        ida_ips_fragmented.append(standard)\n",
    "        skip_standards.append(standard)\n",
    "        if standard[4:] + '_TOF MS' in ida_ips_sorted:\n",
    "            ida_ips_precursor.append(standard[4:] + '_TOF MS')\n",
    "            skip_standards.append(standard[4:] + '_TOF MS')\n",
    "        else:\n",
    "            ida_ips_precursor.append(np.nan)\n",
    "    else:\n",
    "        if 'IDA-' + standard[:-7] in ida_ips_sorted:\n",
    "            ida_ips_precursor.append(standard)\n",
    "            skip_standards.append(standard)\n",
    "            if standard[4:] + '_TOF MS' in ida_ips_sorted:\n",
    "                ida_ips_fragmented.append('IDA-' + standard[:-7])\n",
    "                skip_standards.append('IDA-' + standard[:-7])\n",
    "            else:\n",
    "                ida_ips_fragmented.append(np.nan)\n",
    "        elif 'IPS-' + standard[:-7] in ida_ips_sorted:\n",
    "            ida_ips_precursor.append(standard)\n",
    "            skip_standards.append(standard)\n",
    "            if standard[4:] + '_TOF MS' in ida_ips_sorted:\n",
    "                ida_ips_fragmented.append('IPS-' + standard[:-7])\n",
    "                skip_standards.append('IPS-' + standard[:-7])\n",
    "            else:\n",
    "                ida_ips_fragmented.append(np.nan)\n",
    "        else:\n",
    "            print(f'The standard: {standard} has no corresponding IDA or IPS in the default MS channel. It is ignored in the following calculations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only calibration data\n",
    "data = data[(data['Sample Type'] == 'Standard')]\n",
    "\n",
    "# extract IDA data\n",
    "data_ida = data.loc[(\n",
    "    (data['Component Name'].str.contains('IDA|13C|d-|d3-|d5-')) & ((data['Sample Index']==data['Sample Index'][0]))\n",
    "    ), ['Component Name', 'Component Group Name']]\n",
    "\n",
    "# get rid of IDA and IPS\n",
    "data_components = data[~data['Component Name'].str.contains('IDA|IPS|13C|d-|d3-|d5-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johanna.ganglbauer\\AppData\\Local\\Temp\\ipykernel_23508\\233015985.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  retention_time['retention_time'] = retention_time['Expected RT'] * 60\n"
     ]
    }
   ],
   "source": [
    "# get retention times and deviations\n",
    "retention_time = data[[\n",
    "    'Component Name',  'Expected RT'\n",
    "]]\n",
    "\n",
    "retention_time['retention_time'] = retention_time['Expected RT'] * 60\n",
    "retention_time_final = retention_time.groupby('Component Name', dropna=False).mean()['retention_time'].round(decimals=0)\n",
    "\n",
    "retention_time_check = retention_time.groupby('Component Name', dropna=False).std()['retention_time']\n",
    "retention_time_check.dropna(inplace=True)\n",
    "if any(retention_time_check > 0):\n",
    "    print(f'Retention time seems to be variable.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get precursor mass and fragment mass\n",
    "masses = data[[\n",
    "    'Component Name',  'Precursor Mass', 'Fragment Mass'\n",
    "]]\n",
    "masses_check = masses.groupby('Component Name').std()\n",
    "precursor_check = not all(masses_check['Precursor Mass'].isin([0, np.NaN]) == True)\n",
    "fragment_check = not all(masses_check['Fragment Mass'].isin([0, np.NaN]) == True)\n",
    "if precursor_check or fragment_check:\n",
    "    print('Masses seem to be variable, double check evaluation method')\n",
    "masses = masses.groupby('Component Name', dropna=False).mean()\n",
    "\n",
    "masses['m/z'] = masses['Precursor Mass']\n",
    "masses.loc[~masses.index.str.contains('_TOF MS'), 'm/z'] = masses.loc[~masses.index.str.contains('_TOF MS'), 'Fragment Mass']\n",
    "masses.drop(columns=['Precursor Mass', 'Fragment Mass'], inplace=True)\n",
    "\n",
    "# merge all dataframes\n",
    "channels = masses.merge(retention_time_final, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_data = pd.DataFrame(columns=(\n",
    "    'internal_name',  'precursor_channel_name', 'precursor_m_z_ratio', 'precursor_retention_time',\n",
    "    'fragment_channel_name', 'fragment_m_z_ratio', 'fragment_retention_time',\n",
    "    )\n",
    "    )\n",
    "index = 0\n",
    "mschemicals = {}\n",
    "# loop over component precursors and fragements, collect molecular masses and times and \n",
    "for (component_precursor, component_fragment) in zip(components_precursor + ida_ips_precursor, components_fragmented + ida_ips_fragmented):\n",
    "    \n",
    "    if component_precursor is np.NaN:\n",
    "        precursor_row = pd.Series(data={\n",
    "            'retention_time': np.NaN,\n",
    "            'm/z': np.NaN,\n",
    "        })\n",
    "        component_precursor = np.NaN\n",
    "    else:\n",
    "        internal_name = component_precursor[:-7]\n",
    "        precursor_row = channels.loc[component_precursor, :]\n",
    "\n",
    "    if component_fragment is np.NaN:\n",
    "        fragment_row = pd.Series(data={\n",
    "            'retention_time': np.NaN,\n",
    "            'm/z': np.NaN,\n",
    "        })\n",
    "        component_fragment = np.NaN\n",
    "    else:\n",
    "        internal_name = component_fragment\n",
    "        fragment_row = channels.loc[component_fragment, :]\n",
    "\n",
    "    channel_data.loc[index, 'internal_name'] = internal_name\n",
    "    channel_data.loc[index, 'precursor_channel_name'] = component_precursor\n",
    "    channel_data.loc[index, 'precursor_m_z_ratio'] = precursor_row['m/z']\n",
    "    channel_data.loc[index, 'precursor_retention_time'] = precursor_row['retention_time']\n",
    "    channel_data.loc[index, 'fragment_channel_name'] = component_fragment\n",
    "    channel_data.loc[index, 'fragment_m_z_ratio'] = fragment_row['m/z']\n",
    "    channel_data.loc[index, 'fragment_retention_time'] = fragment_row['retention_time']\n",
    "    mschemicals[internal_name] = pyda.MSChemical(\n",
    "        precursor_channel_name=component_precursor, precursor_m_z_ratio=precursor_row['m/z'],\n",
    "        precursor_retention_time=precursor_row['retention_time'],\n",
    "        fragment_channel_name=component, fragment_m_z_ratio=fragment_row['m/z'],\n",
    "        fragment_retention_time=fragment_row['retention_time'],\n",
    "    )\n",
    "    index += 1\n",
    "\n",
    "channel_data.to_csv('channels.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cas must be supplied",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[156], line 63\u001b[0m\n\u001b[0;32m     61\u001b[0m     components_data\u001b[38;5;241m.\u001b[39mloc[index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mida\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ida_name\n\u001b[0;32m     62\u001b[0m     components_data\u001b[38;5;241m.\u001b[39mloc[index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mips\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ips_name\n\u001b[1;32m---> 63\u001b[0m     list_of_chemicals\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpyda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChemicalSubstance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43minternal_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomponent_precursor\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msmiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreference_line\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mreference_line\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCompound name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpubchem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpubchemlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchemical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchemical\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mida\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mida\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mips\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mips\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstrumentation_detection_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNaN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     70\u001b[0m root \u001b[38;5;241m=\u001b[39m pyda\u001b[38;5;241m.\u001b[39mListOfChemicals(pfas\u001b[38;5;241m=\u001b[39mlist_of_chemicals)\n\u001b[0;32m     72\u001b[0m components_data\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomponents.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m<string>:12\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, internal_name, pubchem, cas, name, smiles, chemical, ida, ips, instrumentation_detection_limit, **_kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\johanna.ganglbauer\\archive\\code\\environmental_sample_data_model\\chemicals.py:125\u001b[0m, in \u001b[0;36mChemicalSubstance.__post_init__\u001b[1;34m(self, *_, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpubchem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpubchem)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_empty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcas):\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMissingRequiredField\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcas, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcas)\n",
      "File \u001b[1;32mc:\\Users\\johanna.ganglbauer\\AppData\\Local\\anaconda3\\envs\\linkml\\Lib\\site-packages\\linkml_runtime\\utils\\yamlutils.py:281\u001b[0m, in \u001b[0;36mYAMLRoot.MissingRequiredField\u001b[1;34m(self, field_name)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mMissingRequiredField\u001b[39m(\u001b[38;5;28mself\u001b[39m, field_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    280\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Generic loader error handler \"\"\"\u001b[39;00m\n\u001b[1;32m--> 281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfield_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be supplied\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: cas must be supplied"
     ]
    }
   ],
   "source": [
    "# make PFAS data frame and add IDA IPS on a component basis\n",
    "basics = data_components[(data_components['Sample Index']==data_components['Sample Index'][0])]\n",
    "\n",
    "components_data = pd.DataFrame(columns=(\n",
    "    'internal_name', 'name', 'pubchem', 'cas', 'smiles',\n",
    "    'ida_name', 'ips_name', 'instrumentation_detection_limit',\n",
    "    )\n",
    "    )\n",
    "\n",
    "# read in references from excel sheet and add them to information\n",
    "references = pd.read_excel('PFAS_STEEP_identifiers.xlsx')\n",
    "list_of_chemicals = []\n",
    "\n",
    "# loop over component precursors, collect all the information for each precursor and put it to dataframe components_data\n",
    "for (index, component_precursor) in enumerate(components_precursor):\n",
    "    # get name of fragemented component and the corresponding information from the reference excel\n",
    "    reference_line = references.loc[references['Abbreviation'].isin([component_precursor[:-7]]), :]\n",
    "    if reference_line.empty:\n",
    "        print(f'There is no data for {component_precursor[:-7]} available.')\n",
    "        reference_line = pd.Series(data={\n",
    "            'Compound name': np.NaN,\n",
    "            'CAS': np.NaN,\n",
    "            'SMILES': np.NaN,\n",
    "        })\n",
    "    chemical = mschemicals[component_precursor[:-7]]\n",
    "\n",
    "    component_fragmented = components_fragmented[index]\n",
    "    # get ida if available\n",
    "    if component_fragmented is not np.NaN:\n",
    "        ida_name = basics.loc[basics['Component Name'].isin([component_fragmented]), 'IS Name'].values[0]\n",
    "        ida = mschemicals[ida_name]\n",
    "    else:\n",
    "        ida_name = np.NaN\n",
    "        ida = np.NaN\n",
    "    \n",
    "    # get ips if available\n",
    "    if ida_name in [np.NaN, 'nan']:\n",
    "        ips_name = np.NaN\n",
    "        ips = np.NaN\n",
    "    else:\n",
    "        ips_name = data_ida.loc[data_ida['Component Name'].isin([ida_name]), 'Component Group Name'].values[0]\n",
    "        ips = mschemicals[ips_name]\n",
    "\n",
    "    components_data.loc[index, 'internal_name'] = component_precursor[:-7]\n",
    "    components_data.loc[index, 'name'] = reference_line.loc[reference_line.index, 'Compound name'].values[0]\n",
    "\n",
    "    pubchemlink = (reference_line.loc[reference_line.index, 'PubChem link']).values[0]\n",
    "    cas = reference_line.loc[reference_line.index, 'CAS'].values[0]\n",
    "    smiles = reference_line.loc[reference_line.index, 'SMILES'].values[0]\n",
    "    if not pubchemlink is np.NaN:\n",
    "        pubchemlink = pubchemlink[42:]\n",
    "\n",
    "    components_data.loc[index, 'pubchem'] = pubchemlink\n",
    "    components_data.loc[index, 'cas'] = cas\n",
    "    components_data.loc[index, 'smiles'] = smiles\n",
    "    components_data.loc[index, 'ida'] = ida_name\n",
    "    components_data.loc[index, 'ips'] = ips_name\n",
    "    list_of_chemicals.append(pyda.ChemicalSubstance(\n",
    "        internal_name=component_precursor[:-7], cas=cas, smiles=smiles,\n",
    "        name=reference_line.loc[reference_line.index, 'Compound name'].values[0],\n",
    "        pubchem=pubchemlink, chemical=chemical , ida=ida, ips=ips,\n",
    "        instrumentation_detection_limit=np.NaN,\n",
    "    ))\n",
    "\n",
    "root = pyda.ListOfChemicals(pfas=list_of_chemicals)\n",
    "\n",
    "components_data.to_csv('components.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data_to_schema(yamlschema: str, data: str, ) -> None:\n",
    "    \"\"\"Validates .yaml data against linkml data schema.\n",
    "\n",
    "    :param yamlschema: Name of .yaml schema.\n",
    "    :type yamlschema: str\n",
    "    :param data: name of data to be validated\n",
    "    :type data: str\n",
    "    \"\"\"  \n",
    "\n",
    "    #[\"linkml-validate -s\", yamlschema, data]\n",
    "    my_env = os.environ.copy()\n",
    "\n",
    "    p = subprocess.Popen(\n",
    "        [\"linkml-validate -s\", yamlschema, data], shell=True, stdout=subprocess.PIPE, env=my_env,\n",
    "        cwd=os.getcwd())\n",
    "    output, error = p.communicate()\n",
    "    if error is not None:\n",
    "        print(error)\n",
    "    else:\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b''\n"
     ]
    }
   ],
   "source": [
    "# Convert data to json\n",
    "yaml_data = yaml_dumper.dumps(root)\n",
    "with open(\"data.yaml\", \"w\") as f:\n",
    "    f.write(yaml_data)\n",
    "\n",
    "# Validate\n",
    "validate_data_to_schema(yamlschema=\"chemicals.yaml\", data=\"data.yaml\")\n",
    "\n",
    "# Convert data to .csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linkml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
