{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from linkml_runtime.dumpers import yaml_dumper\n",
    "import pypandoc as docgenerator\n",
    "from typing import Optional\n",
    "\n",
    "from utils_linkml import(\n",
    "    validate_data_to_schema, convert_data_to_csv,\n",
    "    create_python_schema, generate_schema_png, generate_schema_excel,\n",
    "    generate_schema_documentation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data upload file path\n",
    "raw_filepaths = [\n",
    "    r'C:/Users/johanna.ganglbauer/archive/code/PFAS-SCIEX-Data-Processing-Analysis/example_data_raw/241031_test_data_core.txt',\n",
    "    r'C:/Users/johanna.ganglbauer/archive/code/PFAS-SCIEX-Data-Processing-Analysis/example_data_raw/241031_test_data_extended.txt',\n",
    "]\n",
    "\n",
    "# file paths for IDL and IQL data - not meant to be adopted\n",
    "idl_2024_filepath = r'C:/Users/johanna.ganglbauer/archive/code/PFAS-SCIEX-Data-Processing-Analysis/example_data_raw/IDL_2024.csv'\n",
    "\n",
    "# will be used to identify standards by the column 'Component Name'\n",
    "standard_identifiers = 'IDA|IPS|13C|d-|d3-|d5-|18O'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load linkML data model, get illstrations and python data classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_python_schema(yamlschema='chemicals.yaml', pythonschema='chemicals.py')\n",
    "import chemicals as pyda\n",
    "generate_schema_png(yamlschema='chemicals.yaml', directory='outputs/chemicals')\n",
    "generate_schema_excel(yamlschema='chemicals.yaml', excelschema='outputs/chemicals/schema.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in and data clean up\n",
    "- reads in mass spec data results and uses the calibration data to extract relevant information\n",
    "- reads in excel with associated names and assigns them to PFAS components\n",
    "- read in IDLs and assigns them to PFAS components\n",
    "\n",
    "-> combines the data and saves it to the python data classes deduced from linkml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns of input which are needed for further processes:\n",
    "columns_considered = [\n",
    "    'Sample Name', 'Sample Index', 'Sample Comment', 'Sample Type',\n",
    "    'Component Name',  'Component Group Name', 'IS Name', \n",
    "    'Expected RT',\n",
    "    'Precursor Mass', 'Fragment Mass',\n",
    "]\n",
    "\n",
    "# Load input data files and put them all in one dataframe\n",
    "data = pd.DataFrame()  # initialize empty data frames\n",
    "sample_index = 0  # initialize Component Index\n",
    "for file in raw_filepaths:\n",
    "    # read in file\n",
    "    if file[-4:] == '.csv':\n",
    "        this_data = pd.read_csv(file, delimiter=',', encoding='utf-8', low_memory=False, header=0,)\n",
    "    elif file[-4:] == '.txt':\n",
    "        this_data = pd.read_csv(file, delimiter='\\t', encoding='utf-8', low_memory=False, header=0,)\n",
    "    else:\n",
    "        print('Raw input file paths must either be .csv or .txt files.')\n",
    "\n",
    "    # increase component index to remain unique for multiple data frames\n",
    "    this_data['Sample Index'] = this_data['Sample Index'] + sample_index\n",
    "    sample_index += max(this_data['Sample Index'])\n",
    "    check = this_data['Sample Index'].value_counts()\n",
    "\n",
    "    # make sure each sample name ends with Ext for extended method and with Core for core method\n",
    "    if file[-12:-4] == 'extended':\n",
    "        mask_names = this_data['Sample Name'].str.endswith('Ext')\n",
    "        this_data['Sample Name'][~mask_names] = [compound + ' Ext' for compound in this_data['Sample Name'][~mask_names].to_list()]\n",
    "\n",
    "        # checks if extended data has 107 channels\n",
    "        for index, elem in check[(check != 108)].items():\n",
    "            print(f'The sample with Index {index} has {elem} channels.')\n",
    "\n",
    "    elif file[-8:-4] == 'core':\n",
    "        mask_names = this_data['Sample Name'].str.endswith('Core')\n",
    "        this_data['Sample Name'][~mask_names] = [compound + ' Core' for compound in this_data['Sample Name'][~mask_names].to_list()]\n",
    "\n",
    "        # checks if extended data has 107 channels\n",
    "        for index, elem in check[(check != 142)].items():\n",
    "            print(f'The sample with Index {index} has {elem} channels.')\n",
    "    else:\n",
    "        print('If you combine core method and extended method make sure your input file names end with _core and _extended respectively.')\n",
    "\n",
    "    # append actual dataframe in list (this_data) to huge dataframe (data)\n",
    "    if data.empty:\n",
    "        data = this_data[columns_considered]  # initialize data in first step (when data is empty)\n",
    "    else:\n",
    "        data = pd.concat([data, this_data[columns_considered]], ignore_index=True)  # append to data\n",
    "\n",
    "# Correct channel names in original data (all of the TOF channels are labelled by _TOF MS, only 2 of them are labeled by only _TOF)\n",
    "mask_names = data['Component Name'].str.endswith('_TOF')\n",
    "data['Component Name'][mask_names] = [compound + ' MS' for compound in data['Component Name'][mask_names].to_list()]\n",
    "\n",
    "# some have an underscore between TOF and MS, this is removed\n",
    "mask_names = data['Component Name'].str.endswith('_TOF_MS')\n",
    "data['Component Name'][mask_names] = [compound[:-3] + ' MS' for compound in data['Component Name'][mask_names].to_list()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First make sure that each sample of the core method is assigned correctly to a corresponding sample of the extended method.\\\n",
    "If duplicates of samples with the exact same name are available the assignment works just by the order or the sample index.\n",
    "\n",
    "Then assign each PFAS component detection channels of precursor masses (_TOF MS) to the channel detecting its fragements (default).\n",
    "Currently, some PFAS components are only detected in the _TOF MS channel, then the corresponding fragments are set to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sample index equal if one sample exist as both \"Core Method\" and \"Extended Method\"\n",
    "sample_names = data['Sample Name'].unique()\n",
    "\n",
    "# combine indices of Core Method and Extended Method if both are available\n",
    "detect = 0\n",
    "index_mapper = {}\n",
    "for sample_name in sample_names:\n",
    "    if sample_name[-3:] == 'Ext':\n",
    "        if sample_name[:-3] + 'Core' in sample_names:\n",
    "            sample_indices_core = data.loc[data['Sample Name'].isin([sample_name[:-3] + 'Core']), 'Sample Index'].value_counts().index\n",
    "            sample_indices_extended = data.loc[data['Sample Name'].isin([sample_name]), 'Sample Index'].value_counts().index\n",
    "            if len(sample_indices_core) != len(sample_indices_extended):\n",
    "                print(f'At least one of the sample {sample_name} has no related Core Method. Find out if this is a problem.')\n",
    "            for index_core, index_extended in zip(sample_indices_core, sample_indices_extended):\n",
    "                data.loc[data['Sample Index'].isin([index_core, index_extended]), 'Sample Index'] = index_core\n",
    "                index_mapper[index_core] = sample_name[:-3]\n",
    "                detect+=1\n",
    "        else:\n",
    "            print(f'The sample {sample_name} has no related Core Method. Find out if this is a problem.')\n",
    "    elif sample_name[-4:] == 'Core':\n",
    "        continue\n",
    "    else:\n",
    "        print(f'Make sure your input data name ends either with _core or with _extended.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the order of components and split it to default channel (MS/MS) and TOF channel.\n",
    "# If either channel is not available a copy of the other one is used respectively.\n",
    "first_sample_id = data['Sample Index'].value_counts().index[0]  # index of first sample\n",
    "components_filtered = data.loc[data['Sample Index'] == first_sample_id, 'Component Name']  # channel names of first sample\n",
    "components_sorted = components_filtered[~components_filtered.str.contains(standard_identifiers)].to_list()  # channel names excluding IPS and IDA\n",
    "ida_ips_sorted = components_filtered[components_filtered.str.contains(standard_identifiers)].to_list()  # channel names excluding IPS and IDA\n",
    "\n",
    "# initialize and fill lists of sorted components\n",
    "components_fragmented = []\n",
    "components_precursor = []\n",
    "skip_components = []\n",
    "for component in components_sorted:\n",
    "    if component in skip_components:\n",
    "        continue\n",
    "    if '_TOF MS' in component:\n",
    "        components_precursor.append(component)\n",
    "        skip_components.append(component)\n",
    "        if component[:-7] in components_sorted:\n",
    "            components_fragmented.append(component[:-7])\n",
    "        else:\n",
    "            components_fragmented.append(np.nan)\n",
    "        skip_components.append(component[:-7])\n",
    "    else:\n",
    "        components_fragmented.append(component)\n",
    "        skip_components.append(component)\n",
    "        if component + '_TOF MS' in components_sorted:\n",
    "            components_precursor.append(component + '_TOF MS')\n",
    "        else:\n",
    "            components_precursor.append(np.nan)\n",
    "        skip_components.append(component + '_TOF MS')\n",
    "\n",
    "print(components_fragmented, len(components_fragmented))\n",
    "print(components_precursor, len(components_precursor))\n",
    "\n",
    "# initialize and fill lists of sorted internal standards\n",
    "ida_ips_fragmented = []\n",
    "ida_ips_precursor = []\n",
    "skip_standards = []\n",
    "for standard in ida_ips_sorted:\n",
    "    if standard in skip_standards:\n",
    "        continue\n",
    "    if '_TOF MS' not in standard:\n",
    "        ida_ips_fragmented.append(standard)\n",
    "        skip_standards.append(standard)\n",
    "        if standard[4:] + '_TOF MS' in ida_ips_sorted:\n",
    "            ida_ips_precursor.append(standard[4:] + '_TOF MS')\n",
    "            skip_standards.append(standard[4:] + '_TOF MS')\n",
    "        else:\n",
    "            ida_ips_precursor.append(np.nan)\n",
    "    else:\n",
    "        if 'IDA-' + standard[:-7] in ida_ips_sorted:\n",
    "            ida_ips_precursor.append(standard)\n",
    "            skip_standards.append(standard)\n",
    "            if standard[4:] + '_TOF MS' in ida_ips_sorted:\n",
    "                ida_ips_fragmented.append('IDA-' + standard[:-7])\n",
    "                skip_standards.append('IDA-' + standard[:-7])\n",
    "            else:\n",
    "                ida_ips_fragmented.append(np.nan)\n",
    "        elif 'IPS-' + standard[:-7] in ida_ips_sorted:\n",
    "            ida_ips_precursor.append(standard)\n",
    "            skip_standards.append(standard)\n",
    "            if standard[4:] + '_TOF MS' in ida_ips_sorted:\n",
    "                ida_ips_fragmented.append('IPS-' + standard[:-7])\n",
    "                skip_standards.append('IPS-' + standard[:-7])\n",
    "            else:\n",
    "                ida_ips_fragmented.append(np.nan)\n",
    "        else:\n",
    "            print(f'The standard: {standard} has no corresponding IDA or IPS in the default MS channel. It is ignored in the following calculations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only calibration data\n",
    "data = data[(data['Sample Type'] == 'Standard')]\n",
    "\n",
    "# extract IDA data\n",
    "data_ida = data.loc[(\n",
    "    (data['Component Name'].str.contains(standard_identifiers)) & ((data['Sample Index']==data['Sample Index'][0]))\n",
    "    ), ['Component Name', 'Component Group Name']]\n",
    "\n",
    "# get rid of IDA and IPS\n",
    "data_components = data[~data['Component Name'].str.contains(standard_identifiers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get retention times and deviations\n",
    "retention_time = data[[\n",
    "    'Component Name',  'Expected RT'\n",
    "]]\n",
    "\n",
    "retention_time['retention_time'] = retention_time['Expected RT'] * 60\n",
    "retention_time_final = retention_time.groupby('Component Name', dropna=False).mean()['retention_time'].round(decimals=0)\n",
    "\n",
    "retention_time_check = retention_time.groupby('Component Name', dropna=False).std()['retention_time']\n",
    "retention_time_check.dropna(inplace=True)\n",
    "if any(retention_time_check > 0):\n",
    "    print(f'Retention time seems to be variable.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get precursor mass and fragment mass\n",
    "masses = data[[\n",
    "    'Component Name',  'Precursor Mass', 'Fragment Mass'\n",
    "]]\n",
    "masses_check = masses.groupby('Component Name').std()\n",
    "precursor_check = not all(masses_check['Precursor Mass'].isin([0, np.NaN]) == True)\n",
    "fragment_check = not all(masses_check['Fragment Mass'].isin([0, np.NaN]) == True)\n",
    "if precursor_check or fragment_check:\n",
    "    print('Masses seem to be variable, double check evaluation method')\n",
    "masses = masses.groupby('Component Name', dropna=False).mean()\n",
    "\n",
    "masses['m/z'] = masses['Precursor Mass']\n",
    "masses.loc[~masses.index.str.contains('_TOF MS'), 'm/z'] = masses.loc[~masses.index.str.contains('_TOF MS'), 'Fragment Mass']\n",
    "masses.drop(columns=['Precursor Mass', 'Fragment Mass'], inplace=True)\n",
    "\n",
    "# merge all dataframes\n",
    "channels = masses.merge(retention_time_final, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load idl values from IDL_2024 file\n",
    "idl_2024 = pd.read_csv(idl_2024_filepath, index_col=0, low_memory=False, nrows=1)\n",
    "idl_2024 = idl_2024.drop(columns=['Unnamed: 55'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_data = pd.DataFrame(columns=(\n",
    "    'internal_name',  'precursor_channel_name', 'precursor_m_z_ratio', 'precursor_retention_time',\n",
    "    'fragment_channel_name', 'fragment_m_z_ratio', 'fragment_retention_time',\n",
    "    )\n",
    "    )\n",
    "index = 0\n",
    "mschemicals = {}\n",
    "# loop over component precursors and fragements, collect molecular masses and times and \n",
    "for (component_precursor, component_fragment) in zip(components_precursor + ida_ips_precursor, components_fragmented + ida_ips_fragmented):\n",
    "    \n",
    "    if component_precursor is np.NaN:\n",
    "        precursor_row = pd.Series(data={\n",
    "            'retention_time': np.NaN,\n",
    "            'm/z': np.NaN,\n",
    "        })\n",
    "        component_precursor = np.NaN\n",
    "    else:\n",
    "        internal_name = component_precursor[:-7]\n",
    "        precursor_row = channels.loc[component_precursor, :]\n",
    "\n",
    "    if component_fragment is np.NaN:\n",
    "        fragment_row = pd.Series(data={\n",
    "            'retention_time': np.NaN,\n",
    "            'm/z': np.NaN,\n",
    "        })\n",
    "        component_fragment = np.NaN\n",
    "    else:\n",
    "        internal_name = component_fragment\n",
    "        fragment_row = channels.loc[component_fragment, :]\n",
    "\n",
    "    channel_data.loc[index, 'internal_name'] = internal_name\n",
    "    channel_data.loc[index, 'precursor_channel_name'] = component_precursor\n",
    "    channel_data.loc[index, 'precursor_m_z_ratio'] = precursor_row['m/z']\n",
    "    channel_data.loc[index, 'precursor_retention_time'] = precursor_row['retention_time']\n",
    "    channel_data.loc[index, 'fragment_channel_name'] = component_fragment\n",
    "    channel_data.loc[index, 'fragment_m_z_ratio'] = fragment_row['m/z']\n",
    "    channel_data.loc[index, 'fragment_retention_time'] = fragment_row['retention_time']\n",
    "    mschemicals[internal_name] = pyda.MSChemical(\n",
    "        precursor_channel_name=component_precursor, precursor_m_z_ratio=precursor_row['m/z'],\n",
    "        precursor_retention_time=precursor_row['retention_time'],\n",
    "        fragment_channel_name=component, fragment_m_z_ratio=fragment_row['m/z'],\n",
    "        fragment_retention_time=fragment_row['retention_time'],\n",
    "    )\n",
    "    index += 1\n",
    "\n",
    "channel_data.to_csv('outputs//chemicals//channels.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make PFAS data frame and add IDA IPS on a component basis\n",
    "basics = data_components[(data_components['Sample Index']==data_components['Sample Index'][0])]\n",
    "\n",
    "components_data = pd.DataFrame(columns=(\n",
    "    'internal_name', 'name', 'pubchem', 'cas', 'smiles',\n",
    "    'ida', 'ips', 'instrumentation_detection_limit',\n",
    "    )\n",
    "    )\n",
    "\n",
    "# read in references from excel sheet and add them to information\n",
    "references = pd.read_excel('PFAS_STEEP_identifiers.xlsx')\n",
    "\n",
    "# create dictionary mapping indices to component names and abbreviations\n",
    "abbreviations_dict = {}\n",
    "for index in references.index:\n",
    "    abbreviation = [references.loc[index, 'Abbreviation']]\n",
    "    synonyms = references.loc[index, 'Synonyms']\n",
    "    if synonyms is not np.nan:\n",
    "        for synonym in str(synonyms).split(','):\n",
    "            abbreviation.append(synonym)\n",
    "    abbreviations_dict[index] = abbreviation\n",
    "\n",
    "# initialize variable list of chemicals\n",
    "list_of_chemicals = []\n",
    "\n",
    "# loop over component precursors, collect all the information for each precursor and put it to dataframe components_data\n",
    "for (component_index, component_precursor) in enumerate(components_precursor):\n",
    "    # get name of fragemented component, the corresponding index in the reference excel by abbreviations and synonyms and extract relevant row from the excel\n",
    "    for reference_index, abbreviations in abbreviations_dict.items():\n",
    "        if component_precursor[:-7] in abbreviations:\n",
    "            reference_line = references.iloc[reference_index, :]\n",
    "            break\n",
    "    \n",
    "    # fill empty data in name is not available\n",
    "    if reference_line.empty:\n",
    "        print(f'There is no data for {component_precursor[:-7]} available.')\n",
    "        reference_line = pd.Series(data={\n",
    "            'Compound name': np.NaN,\n",
    "            'CAS': np.NaN,\n",
    "            'SMILES': np.NaN,\n",
    "        })\n",
    "    chemical = mschemicals[component_precursor[:-7]]\n",
    "\n",
    "    component_fragmented = components_fragmented[component_index]\n",
    "    # get ida if available\n",
    "    if component_fragmented is not np.NaN:\n",
    "        ida_name = basics.loc[basics['Component Name'].isin([component_fragmented]), 'IS Name'].values[0]\n",
    "        ida = mschemicals[ida_name]\n",
    "    else:\n",
    "        ida_name = np.NaN\n",
    "        ida = np.NaN\n",
    "    \n",
    "    # get ips if available\n",
    "    if ida_name in [np.NaN, 'nan']:\n",
    "        ips_name = np.NaN\n",
    "        ips = np.NaN\n",
    "    else:\n",
    "        ips_name = data_ida.loc[data_ida['Component Name'].isin([ida_name]), 'Component Group Name'].values[0]\n",
    "        ips = mschemicals[ips_name]\n",
    "\n",
    "    if component_precursor[:-7] in idl_2024.columns:\n",
    "        idl = idl_2024.loc['IDL', component_precursor[:-7]]\n",
    "    else:\n",
    "        idl = np.NaN\n",
    "\n",
    "    components_data.loc[component_index, 'internal_name'] = component_precursor[:-7]\n",
    "    components_data.loc[component_index, 'name'] = reference_line.loc['Compound name']\n",
    "\n",
    "    pubchemlink = (reference_line.loc['PubChem link'])\n",
    "    cas = reference_line.loc['CAS']\n",
    "    smiles = reference_line.loc['SMILES']\n",
    "    if not pubchemlink is np.NaN:\n",
    "        pubchemlink = pubchemlink[42:]\n",
    "\n",
    "    components_data.loc[component_index, 'pubchem'] = pubchemlink\n",
    "    components_data.loc[component_index, 'cas'] = cas\n",
    "    components_data.loc[component_index, 'smiles'] = smiles\n",
    "    components_data.loc[component_index, 'ida'] = ida_name\n",
    "    components_data.loc[component_index, 'ips'] = ips_name\n",
    "    components_data.loc[component_index, 'instrumentation_detection_limit'] = idl\n",
    "    list_of_chemicals.append(pyda.ChemicalSubstance(\n",
    "        internal_name=component_precursor[:-7], cas=cas, smiles=smiles,\n",
    "        name=reference_line.loc['Compound name'], pubchem=pubchemlink,\n",
    "        chemical=chemical , ida=ida, ips=ips, instrumentation_detection_limit=idl,\n",
    "    ))\n",
    "\n",
    "root = pyda.ListOfChemicals(pfas=list_of_chemicals)\n",
    "\n",
    "components_data.to_csv('outputs//chemicals//components.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to json\n",
    "yaml_data = yaml_dumper.dumps(root)\n",
    "with open(\"outputs/chemicals/data.yaml\", \"w\") as f:\n",
    "    f.write(yaml_data)\n",
    "\n",
    "# Validate\n",
    "validate_data_to_schema(yamlschema=\"chemicals.yaml\", yamldata=\"outputs/chemicals/data.yaml\")\n",
    "\n",
    "# Convert data to .csv\n",
    "convert_data_to_csv(\n",
    "    yamlschema=\"chemicals.yaml\", yamldata=\"outputs/chemicals/data.yaml\", csvdata='outputs/chemicals/data.csv',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate documentation\n",
    "generate_schema_documentation(\n",
    "    yamlschema='chemicals.yaml', directory='outputs/chemicals', example_directory='outputs/chemicals/data.yaml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linkml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
